{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "import time\n",
    "import pyautogui\n",
    "import subprocess\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Start frame recorded!\n",
      "‚úÖ Mid1 frame recorded!\n",
      "‚úÖ Mid2 frame recorded!\n",
      "‚úÖ End frame recorded!\n",
      "üìÅ Sign 'mute' saved permanently!\n",
      "üü¢ 'Start' frame matched for 'mute'!\n",
      "‚è∞ Stage timed out. Resetting sequence.\n",
      "üü¢ 'Start' frame matched for 'volume_up'!\n",
      "üü¢ 'Mid1' frame matched for 'volume_up'!\n",
      "üü¢ 'Mid2' frame matched for 'volume_up'!\n",
      "‚úÖ Detected Sign: volume_up\n",
      "üü¢ 'Start' frame matched for 'volume_down'!\n",
      "üü¢ 'Mid1' frame matched for 'volume_down'!\n",
      "üü¢ 'Mid2' frame matched for 'volume_down'!\n",
      "‚úÖ Detected Sign: volume_down\n",
      "üü¢ 'Start' frame matched for 'son_2'!\n",
      "üü¢ 'Mid1' frame matched for 'son_2'!\n",
      "‚è∞ Stage timed out. Resetting sequence.\n",
      "üü¢ 'Start' frame matched for 'close_window'!\n",
      "üü¢ 'Mid1' frame matched for 'close_window'!\n",
      "üü¢ 'Mid2' frame matched for 'close_window'!\n",
      "‚úÖ Detected Sign: close_window\n",
      "üü¢ 'Start' frame matched for 'close_window'!\n",
      "‚è∞ Stage timed out. Resetting sequence.\n",
      "‚úÖ Start frame recorded!\n",
      "‚úÖ Mid1 frame recorded!\n",
      "‚úÖ Mid2 frame recorded!\n",
      "‚úÖ End frame recorded!\n",
      "üìÅ Sign 'mute' saved permanently!\n",
      "üü¢ 'Start' frame matched for 'mute'!\n",
      "üü¢ 'Mid1' frame matched for 'mute'!\n",
      "üü¢ 'Mid2' frame matched for 'mute'!\n",
      "‚úÖ Detected Sign: mute\n",
      "üü¢ 'Start' frame matched for 'son_2'!\n",
      "‚è∞ Stage timed out. Resetting sequence.\n",
      "üü¢ 'Start' frame matched for 'open_chrome'!\n",
      "üü¢ 'Mid1' frame matched for 'open_chrome'!\n",
      "üü¢ 'Mid2' frame matched for 'open_chrome'!\n",
      "‚úÖ Detected Sign: open_chrome\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid1' frame matched for 'pause_video'!\n",
      "üü¢ 'Mid2' frame matched for 'pause_video'!\n",
      "‚úÖ Detected Sign: pause_video\n",
      "üü¢ 'Start' frame matched for 'seek_backward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_backward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_backward'!\n",
      "‚úÖ Detected Sign: seek_backward\n",
      "üü¢ 'Start' frame matched for 'seek_backward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_backward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_backward'!\n",
      "‚úÖ Detected Sign: seek_backward\n",
      "üü¢ 'Start' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_forward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_forward'!\n",
      "‚úÖ Detected Sign: seek_forward\n",
      "üü¢ 'Start' frame matched for 'seek_backward'!\n",
      "üü¢ 'Mid1' frame matched for 'seek_backward'!\n",
      "üü¢ 'Mid2' frame matched for 'seek_backward'!\n",
      "‚úÖ Detected Sign: seek_backward\n"
     ]
    }
   ],
   "source": [
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "TARGET_WIDTH = 320\n",
    "TARGET_HEIGHT = 240\n",
    "cap = cv2.VideoCapture(2)\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# File to store gestures\n",
    "GESTURE_FILE = \"gestures.json\"\n",
    "COOLDOWN_TIME = 1  # Cooldown time before detecting the next gesture\n",
    "\n",
    "# Define key landmarks to check\n",
    "keypoints_to_check = [0, 1, 4, 5, 8, 9, 12, 13, 16, 17, 20]\n",
    "\n",
    "GESTURE_ACTIONS = {\n",
    "    \"pause_video\": lambda: pyautogui.press('space'),\n",
    "    \"volume_up\": lambda: [pyautogui.press('volumeup') for _ in range(3)],\n",
    "    \"volume_down\": lambda: [pyautogui.press('volumedown') for _ in range(3)],\n",
    "    \"mute\": lambda: pyautogui.press('volumemute'),\n",
    "    \"seek_forward\": lambda: pyautogui.hotkey('right'),\n",
    "    \"seek_backward\": lambda: pyautogui.hotkey('left'),\n",
    "    \"brightness_up\": lambda: pyautogui.press('brightnessup'),\n",
    "    \"brightness_down\": lambda: pyautogui.press('brightnessdown'),\n",
    "\n",
    "    \"open_chrome\": lambda: subprocess.Popen(r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"),\n",
    "    \"close_window\": lambda: pyautogui.hotkey('alt', 'f4'),\n",
    "    \"screenshot\": lambda: pyautogui.hotkey('win', 'printscreen'),\n",
    "}\n",
    " \n",
    "\n",
    "# Load existing gestures if available\n",
    "if os.path.exists(GESTURE_FILE):\n",
    "    with open(GESTURE_FILE, \"r\") as f:\n",
    "        gesture_dict = json.load(f)\n",
    "        # Convert each stored frame into a numpy array\n",
    "        gesture_dict = {\n",
    "            k: {\n",
    "                \"start\": np.array(v[\"start\"], dtype=np.float32),\n",
    "                \"mid1\": np.array(v[\"mid1\"], dtype=np.float32),\n",
    "                \"mid2\": np.array(v[\"mid2\"], dtype=np.float32),\n",
    "                \"end\": np.array(v[\"end\"], dtype=np.float32)\n",
    "            }\n",
    "            for k, v in gesture_dict.items()\n",
    "        }\n",
    "else:\n",
    "    gesture_dict = {}\n",
    "\n",
    "# Variables to track sequential matching\n",
    "pending_gesture = None  # The gesture name matched at \"start\"\n",
    "frame_stage = 0         # 0: start, 1: mid1, 2: mid2, 3: end\n",
    "last_detection_time = 0\n",
    "gesture_keyframes={}\n",
    "\n",
    "# New: Stage timeout (in seconds)\n",
    "STAGE_TIMEOUT = 5\n",
    "stage_start_time = None  # Record when the current stage started\n",
    "\n",
    "def normalize_landmarks(landmarks):\n",
    "    \"\"\"Normalize landmarks by centering and scaling relative to the entire hand size.\"\"\"\n",
    "    min_x, min_y, _ = np.min(landmarks, axis=0)\n",
    "    max_x, max_y, _ = np.max(landmarks, axis=0)\n",
    "    center_x = (min_x + max_x) / 2\n",
    "    center_y = (min_y + max_y) / 2\n",
    "    centered_landmarks = landmarks - np.array([center_x, center_y, 0])\n",
    "    hand_width, hand_height = max_x - min_x, max_y - min_y\n",
    "    scale = max(hand_width, hand_height)\n",
    "    if scale > 0:\n",
    "        centered_landmarks /= scale\n",
    "    return centered_landmarks\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (TARGET_WIDTH, TARGET_HEIGHT))\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Optional cooldown: skip processing if too soon after last detection\n",
    "    if current_time - last_detection_time < COOLDOWN_TIME:\n",
    "        cv2.imshow(\"Sign Prediction\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    # If we're in a sequence (frame_stage > 0) but too much time has passed, reset sequence.\n",
    "    if frame_stage > 0 and stage_start_time is not None:\n",
    "        if current_time - stage_start_time > STAGE_TIMEOUT:\n",
    "            print(\"‚è∞ Stage timed out. Resetting sequence.\")\n",
    "            pending_gesture = None\n",
    "            frame_stage = 0\n",
    "            stage_start_time = None\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = np.array([(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark],\n",
    "                                 dtype=np.float32)\n",
    "            if landmarks.shape[0] < max(keypoints_to_check):\n",
    "                continue  # Incomplete detection; skip this frame\n",
    "\n",
    "            normalized_landmarks = normalize_landmarks(landmarks)\n",
    "            normalized_keypoints = normalized_landmarks[keypoints_to_check]\n",
    "            frame_sequence = [\"start\", \"mid1\", \"mid2\", \"end\"]\n",
    "\n",
    "            # For the first stage, if no sequence is started, try to match \"start\"\n",
    "            if frame_stage == 0:\n",
    "                for gesture_name, frames in gesture_dict.items():\n",
    "                    keyframe_points = frames[\"start\"].reshape(-1, 3)\n",
    "                    if keyframe_points.shape != normalized_keypoints.shape:\n",
    "                        continue\n",
    "                    distance, _ = fastdtw(keyframe_points, normalized_keypoints, dist=euclidean)\n",
    "                    if distance < 1:\n",
    "                        pending_gesture = gesture_name\n",
    "                        frame_stage = 1\n",
    "                        stage_start_time = current_time  # Start timing this stage\n",
    "                        print(f\"üü¢ 'Start' frame matched for '{gesture_name}'!\")\n",
    "                        break\n",
    "\n",
    "            # For subsequent stages, use the pending gesture's corresponding frame\n",
    "            elif pending_gesture:\n",
    "                stage_name = frame_sequence[frame_stage]\n",
    "                keyframe_points = gesture_dict[pending_gesture][stage_name].reshape(-1, 3)\n",
    "                if keyframe_points.shape == normalized_keypoints.shape:\n",
    "                    distance, _ = fastdtw(keyframe_points, normalized_keypoints, dist=euclidean)\n",
    "                    if distance < 1:\n",
    "                        print(f\"üü¢ '{stage_name.capitalize()}' frame matched for '{pending_gesture}'!\")\n",
    "                        frame_stage += 1\n",
    "                        stage_start_time = current_time  # Reset timer for next stage\n",
    "                        if frame_stage == 3:  # All stages matched\n",
    "                            print(f\"‚úÖ Detected Sign: {pending_gesture}\")\n",
    "                            try:\n",
    "                                GESTURE_ACTIONS[pending_gesture]()\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ùå Error executing action: {e}\")\n",
    "                            last_detection_time = current_time\n",
    "                            pending_gesture = None\n",
    "                            frame_stage = 0  # Reset for next gesture detection\n",
    "                            stage_start_time = None\n",
    "                            break  # Stop processing further for this hand\n",
    "\n",
    "    # Handle key presses to record frames or quit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('1'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"start\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ Start frame recorded!\")\n",
    "    elif key == ord('2'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"mid1\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ Mid1 frame recorded!\")\n",
    "    elif key == ord('3'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"mid2\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ Mid2 frame recorded!\")\n",
    "    elif key == ord('4'):\n",
    "        if results.multi_hand_landmarks:\n",
    "            gesture_keyframes[\"end\"] = normalize_landmarks(landmarks)[keypoints_to_check].copy()\n",
    "            print(\"‚úÖ End frame recorded!\")\n",
    "            gesture_name = input(\"Enter the word for this sign: \")\n",
    "            gesture_dict[gesture_name] = gesture_keyframes.copy()\n",
    "            with open(GESTURE_FILE, \"w\") as f:\n",
    "                json.dump(\n",
    "                    {k: {frame: v.tolist() for frame, v in frames.items()} for k, frames in gesture_dict.items()},\n",
    "                    f\n",
    "                )\n",
    "            print(f\"üìÅ Sign '{gesture_name}' saved permanently!\")\n",
    "    elif key == ord('q') or key == 27:\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Sign Prediction\", frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
